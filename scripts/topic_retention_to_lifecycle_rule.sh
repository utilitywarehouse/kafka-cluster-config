#!/usr/bin/env bash

root_cluster=$1
S3_PREFIX="msk-backup-parquet"
OUTPUT_DIR="${root_cluster}/msk-backup-bucket-retention"
OUTPUT_FILE="${OUTPUT_DIR}/retention.tf"

TMP=$(mktemp)

# Extract topic=days
find ${root_cluster} -name "*.tf" | xargs awk '
  /resource "kafka_topic"/ { t=$3; gsub(/"/,"",t) }
  /"retention.ms"/ {
    ms=$3; gsub(/[^0-9]/,"",ms)
    d=int(ms/86400000)
    if(d>0) print t "=" d
  }
' > "$TMP"

# Create output directory if it doesn't exist
mkdir -p "$OUTPUT_DIR"

# Merge by retention and print lifecycle rules
awk -F= -v s3_prefix="$S3_PREFIX" '
{
  days=$2
  topics[days] = topics[days] ? topics[days]" "$1 : $1
}
END {
  # Print header
  print "#################################"
  print "# DO NOT update this file manually, it is autogenerated"
  print "#################################"
  print ""
  print "resource \"aws_s3_bucket_lifecycle_configuration\" \"msk_topics_retention\" {"
  print "  bucket = \"uw-dev-pubsub-msk-backup\""
  print ""

  # Collect and sort retention days ascending
  n=0
  for(d in topics) days_arr[++n]=d
  for(i=1;i<=n;i++){
    for(j=i+1;j<=n;j++){
      if(days_arr[i]+0>days_arr[j]+0){tmp=days_arr[i];days_arr[i]=days_arr[j];days_arr[j]=tmp}
    }
  }

  for(i=1;i<=n;i++){
    d=days_arr[i]
    split(topics[d], arr, " ")
    # Manual bubble sort instead of asort()
    arr_len=length(arr)
    for(m=1;m<=arr_len;m++){
      for(k=m+1;k<=arr_len;k++){
        if(arr[m]>arr[k]){tmp_arr=arr[m];arr[m]=arr[k];arr[k]=tmp_arr}
      }
    }
    printf "  rule {\n"
    printf "    id     = \"retention_%d\"\n", d
    printf "    status = \"Enabled\"\n"
    printf "    expiration { days = %d }\n", d
    for(j=1;j<=arr_len;j++){
      s3_path=arr[j]; gsub("_",".",s3_path)
      printf "    filter { prefix = \"%s/%s/\" }\n", s3_prefix, s3_path
    }
    print "  }"
  }

  # Print closing brace
  print "}"
}' "$TMP" > "$OUTPUT_FILE"

rm "$TMP"

echo "Generated: $OUTPUT_FILE"
