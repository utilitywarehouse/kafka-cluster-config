#!/usr/bin/env bash
#
# This script generates a Terraform S3 bucket lifecycle configuration for MSK
# topic backups based on the retention settings defined in Kafka topic
# resource files.
#
# Usage: ./your_script_name.sh <dev|prod>

# --- START PRE-FLIGHT CHECKS ---
# Exit immediately if a command exits with a non-zero status.
set -e
# Treat unset variables as an error when substituting.
set -u
# Pipelines return the exit status of the last command to exit with a non-zero status.
set -o pipefail

# --- GLOBAL SETUP ---
# Determine the absolute path of the directory where the script is located.
readonly SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )

# Create a temporary file and set a trap to clean it up on script exit.
TMP_FILE=$(mktemp)
trap 'rm -f -- "$TMP_FILE"' EXIT

# --- FUNCTION DEFINITIONS ---

##
# Finds all kafka topic resources, extracts retention data, sorts it,
# and saves it to a temporary file.
# Globals:
#   TMP_FILE
# Arguments:
#   $1: The root directory to search for Terraform files.
##
extract_topic_retention_data() {
  local root_cluster="$1"

  # Find, extract, and sort the data before writing it to the temp file.
  # Use LC_ALL=C to ensure a consistent, portable sort order across systems.
  find "${root_cluster}" -name "*.tf" -print0 | xargs -0 awk '
    BEGIN {
      total_resources=0
      infinite_topics=0
      no_retention_topics=0
      empty_name_topics=0
    }
    /resource "kafka_topic"/ {
      in_resource=1
      topic=""
      retention_ms=""
      resource_name=$3
      gsub(/"/,"",resource_name)
    }
    in_resource && /name[ ]*=/ {
      topic=$3
      gsub(/"/,"",topic)
    }
    in_resource && /"retention.ms"/ {
      retention_ms=$3
      gsub(/[^0-9-]/,"",retention_ms)
    }
    in_resource && /^}/ {
      # Process at end of resource block
      total_resources++
      if(topic == "") {
        empty_name_topics++
        print "WARNING: Empty topic name for resource: " resource_name > "/dev/stderr"
      }
      if(retention_ms == "") {
        no_retention_topics++
      } else if(retention_ms+0 <= 0) {
        infinite_topics++
      } else {
        # Valid retention with positive value
        d=int(retention_ms/86400000) + 1
        if(topic!="") {
          print topic "=" d
        }
      }
      in_resource=0
      topic=""
      retention_ms=""
    }
    END {
      print "\n=== Statistics ===" > "/dev/stderr"
      print "Total kafka_topic resources found: " total_resources > "/dev/stderr"
      print "Topics with infinite retention (retention.ms <= 0): " infinite_topics > "/dev/stderr"
      print "Topics without retention.ms (compacted topics): " no_retention_topics > "/dev/stderr"
      print "Topics with empty name: " empty_name_topics > "/dev/stderr"
      print "Topics with associated bucket rules: " (total_resources - infinite_topics - no_retention_topics) > "/dev/stderr"
    }
  ' | LC_ALL=C sort > "$TMP_FILE"
}

##
# Writes the static Terraform header to the output file.
# Arguments:
#   $1: The output file path.
#   $2: The environment name (e.g., "dev" or "prod").
##
output_header() {
  local output_file="$1"
  local env="$2"

  cat << EOF > "$output_file"
#################################
# DO NOT update this file manually, it is autogenerated
#################################

resource "aws_s3_bucket_lifecycle_configuration" "msk_topics_retention" {
  bucket = "uw-${env}-pubsub-msk-backup"

  rule {
    id     = "default-to-intelligent-tiering"
    status = "Enabled"
    filter {}
    transition {
      days          = 0
      storage_class = "INTELLIGENT_TIERING"
    }
  }

EOF
}

##
# Generates and appends Terraform rules to the output file from pre-sorted data.
# Globals:
#   TMP_FILE
# Arguments:
#   $1: The output file path.
#   $2: The S3 prefix for the backup files.
##
output_rules() {
  local output_file="$1"
  local s3_prefix="$2"

  # Append rules by processing the pre-sorted temp file line-by-line.
  awk -F= -v s3_prefix="$s3_prefix" '
  {
    topic=$1
    days=$2
    s3_path=topic; gsub("_",".",s3_path)
    printf "  rule {\n"
    printf "    id     = \"%s\"\n", topic
    printf "    status = \"Enabled\"\n"
    printf "    expiration { days = %d }\n", days
    printf "    filter { prefix = \"%s/%s/\" }\n", s3_prefix, s3_path
    printf "  }\n\n"
  }
  ' "$TMP_FILE" >> "$output_file"

  # Append the final closing brace.
  echo "}" >> "$output_file"
}

# --- MAIN LOGIC ---
main() {
  local env="$1"
  local root_cluster="${SCRIPT_DIR}/../${env}-aws/kafka-shared-msk"
  local s3_prefix="msk-backup-parquet"
  local output_dir="${root_cluster}/msk-backup-bucket-retention"
  local output_file="${output_dir}/generated-retention.tf"

  # Step 1: Extract and sort data into a temporary file.
  extract_topic_retention_data "$root_cluster"

  # Step 2: Generate the Terraform configuration file.
  mkdir -p "$output_dir"
  output_header "$output_file" "$env"
  output_rules "$output_file" "$s3_prefix"

  echo "Generated: $output_file"
}

# --- SCRIPT EXECUTION ---
# Validate input first, then run main.
if [[ "$#" -ne 1 || ("$1" != "dev" && "$1" != "prod") ]]; then
  echo "Usage: $0 <dev|prod>" >&2
  echo "Error: Please provide a single environment argument: 'dev' or 'prod'." >&2
  exit 1
fi

main "$@"
